% !TEX root = ../thesis.tex

\section{Background}
\label{sec:background}
The following chapter intends to provide a methodological background for two key
concepts underlying the work presented in this thesis, namely
\nameref{subsec:feature_extraction} and the \nameref{subsec:som}.

\subsection{Audio Feature Extraction}
\label{subsec:feature_extraction}
Audio Feature Extraction is the process of deriving \textit{features} from a
digital audio signal. A feature represents some sort of descriptive information
about the audio data. According to \citet{lerch2012}, this extraction process
serves a dual purpose; that of dimensionality reduction as well as a more
meaningful representation. A large variety of features for different purposes
have been developed (refer to \citet{peeters2004} for an extensive list, as well
as \citet{lerch2012} for an in-depth look at the topic). The following
subsections introduce the features used in this work, starting with the
pre-processing required to prepare an audio signal for feature extraction and
then moving into individual definitions for each feature. The equations
presented here are based on the formal definitions given by \citet{lerch2012}
along with the computational implementations of the features in the
\textit{Meyda} library for feature extraction in JavaScript
\citep{rawlinson2015}, which in turn adapted the \textit{yaafe} library for
Python \citep{mathieu2010}.

\subsubsection{Audio Pre-Processing}
\label{subsubsec:preprocessing}
Consider a digital audio signal of the form $x[n]$, where $n$ denotes the sample
index and $x[n]$ the value of the individual sample at that index.

\paragraph*{Normalization}
\label{para:normalization}
In order to have a standardized maximum amplitude of 1 across all audio signals,
they are normalized such that
\begin{equation}
  x_{norm}[n] = \frac{x[n]}{\max{x[n]}}.
\end{equation}

\paragraph*{Mono Conversion}
\label{para:mono_conversion}
Spatial information, as contained in an audio file with more than one channel,
is not deemed necessary in the presented work. For this reason, all audio
signals are converted to mono by taking the average of all channels.

\paragraph*{Frame-Based Feature Extraction}
\label{para:frame_based_extraction}
Rather than performing feature extraction on the entirety of the audio signal,
it is common practice to divide the signal into smaller chunks or
\textit{frames}, typically consisting of some $2^n$ samples (512, 1024, 2048
are often found values). The resulting feature values for each frame form a
trajectory of the feature's evolution over time, which can either be used as
such or can be averaged. In this work, audio signals are divided into frames
with a length of 512 samples.
In order to avoid computational errors (such as \gls{nan} in JavaScript) during
potentially silent portions of the audio signal, frames with
$v_{RMS} < \SI{-60}{dBFS}$ (see \nameref{para:rms} definition below) are omitted
from the feature extraction. The following equations define each feature for a
single frame.

\subsubsection{Time Domain Features}
\label{subsubsec:temporal_features}
% TODO
Time domain features are features derived directly from the discrete-time
signal $x[n]$.

\paragraph*{Duration}
\label{para:duration}
The overall duration of the signal $x[n]$ in seconds:
\begin{equation}
  v_{DUR} = \frac{n}{fs}s,
\end{equation}
where $n$ is the number of samples and $fs$ is the sampling rate.

\paragraph*{Root Mean Square (RMS)}
\label{para:rms}
measures the power of a signal \citep[p.73f]{lerch2012}. It describes sound intensity and is sometimes used as a simple measure for loudness
\citep{web:meyda2019_features} that does not take the nonlinearity of human
hearing into account \citep{fletcher1933}. It is calculated for an audio
frame $x[n]$ consisting of $n$ samples such that
\begin{equation}
  v_{RMS} = \sqrt{ \frac{ \sum\limits_{i=1}^{n} x(i)^2} {n}}.
\end{equation}

\paragraph*{Zero-Crossing Rate (ZCR)}
\label{para:zcr}
represents the rate of the number of sign changes in a signal.
It can be used as a measure of the tonalness of a sound \citep{lykartsis2014}
and as a simple pitch detection method for monophonic signals \citep{web:pitchdetection2019}. It is defined as
\begin{equation}
  v_{ZCR} = \frac{1} {2\cdot{n}}\sum\limits_{i=1}^{n} |sgn[x(i)] - sgn[x(i - 1)]|.
\end{equation}

\subsubsection{Frequency Domain Features}
\label{subsubsec:spectral_features}
% TODO
Frequency domain features or \textit{spectral} features are derived from the
discrete complex spectrum $X(k)$, where $k$ refers to the frequency bin number.
$X(k)$ is calculated from $x[n]$ by performing an \gls{fft} (for information on
the Fourier transform, refer to any signal processing textbook, such as
\citet{oppenheim2014}).

\paragraph*{Spectral Centroid}
\label{para:centroid}
is a measure of the center of gravity of a
spectrum. A higher value indicates a brighter, sharper sound \citep{lerch2012}.
The spectral centroid is defined as
\begin{equation}
  v_{SC} = \frac{ \sum\limits_{k=0}^{N_{FFT}/2-1} k\cdot{|X(k)]^2} }
  { \sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)]^2 }.
\end{equation}

\paragraph*{Spectral Flatness}
\label{para:flatness}
is a measure for the tonality or noisiness of a signal, defined as the ratio of
the geometric and arithmetic means of its magnitude spectrum. Higher values
indicate a flatter (and therefore noisier) spectrum, whereas lower values point
towards more tonal spectral content. It is defined as
\begin{equation}
  v_{SFL} = \frac{ \sqrt[N_{FFT}/2]{ \prod\limits_{k=0}^{N_{FFT}/2-1} |X(k)| } }
  { (2/N_{FFT}) \cdot{\sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)|} }.
\end{equation}

\paragraph*{Spectral Kurtosis}
\label{para:kurtosis}
indicates whether a given magnitude spectrum's distribution is similar to a
Gaussian distribution. Negative values result from a flatter distribution,
whereas positive values indicate a peakier distribution. A Gaussian
distribution would result in a value of 0. Spectral Kurtosis is defined as
\begin{equation}
  v_{SKU} = \frac{ 2 \sum\limits_{k=0}^{N_{FFT}/2-1} (|X(k)|
  - \mu_{|X|})^4 }
  { N_{FFT}\cdot{\sigma_{|X|}^4} } - 3,
\end{equation}

where $\mu_{|X|}$ represents the mean and $\sigma_{|X|}$ the standard deviation
of the magnitude spectrum $|X|$.

\paragraph*{Spectral Skewness}
\label{para:skewness}
assesses the symmetry of a magnitude spectrum distribution. It is defined as
\begin{equation}
  v_{SSK} = \frac{ 2 \sum\limits_{k=0}^{N_{FFT}/2-1} (|X(k)|
  - \mu_{|X|})^3 }
  { N_{FFT}\cdot{\sigma_{|X|}^3} }.
\end{equation}

\paragraph*{Spectral Slope}
\label{para:slope}
represents a measure of how sloped or inclined a given spectral distribution is.
The spectral slope is calculated using a linear regression of the magnitude
spectrum such that
\begin{equation}
  v_{SSL} = \frac{ \sum\limits_{k=0}^{N_{FFT}/2-1} (k - \mu_{k}) (|X(k)|
  - \mu_{|X|}) }
  { \sum\limits_{k=0}^{N_{FFT}/2-1} (k - \mu_{k})^2 }.
\end{equation}

\paragraph*{Spectral Spread}
\label{para:spread}
is a descriptor of the concentration of a magnitude spectrum around the
\nameref{para:centroid} and assesses the corresponding signal's bandwidth. It is
defined as
\begin{equation}
  v_{SSP} = \frac{ \sum\limits_{k=0}^{N_{FFT}/2-1} (k - v_{SC})^2
  \cdot{ |X(k)|^2 } }
  { \sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)|^2 }.
\end{equation}

\paragraph*{Spectral Rolloff}
\label{para:rolloff}
measures the bandwidth of a given signal by calculating that frequency bin
below which lie $\kappa$ percent of the sum of magnitudes of $X(k)$. Common
values for $\kappa$ are $0.85$, $0.95$ \citep{lerch2012} or $0.99$
\citep{web:meyda2019_features}. It is defined as
\begin{equation}
  v_{SR} = i \Bigg\rvert_{ \sum\limits_{k=0}^{i} |X(k)| = \kappa
  \cdot{ \sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)| } }.
\end{equation}

\subsubsection{Perceptual Features}
\label{perceptual_features}
Both the time and frequency domain features introduced above are derived from
raw audio samples without taking into account any concept of human sound
perception. Perceptual features incorporate some sort of model that approximates
this perception. While only a single perceptual feature is used in this work,
more do exist (see \citet{peeters2004} for a list of some of them).

\paragraph*{Total Loudness}
\label{para:loudness}
represents an algorithmic approximation of the human perception of a signal's
loudness based on \citet{moore1997}, which uses the Bark scale as introduced by
\citet{zwicker1961}. The Total Loudness is the sum of all 24 bands' specific
loudness
coefficients, defined by \citet{peeters2004} as
\begin{equation}
  v_{TL} = \sum\limits_{i=1}^{24} v_{SL}(i),
\end{equation}
where
\begin{equation}
  v_{SL}(i) = E(i)^{0.23}
\end{equation}
is the specific loudness of each Bark band (see \citet{moore1997} for further
details).

\subsection{Self-Organizing Map}
\label{subsec:som}

The \textit{self-organizing map} (\gls{som}) is a machine learning algorithm for
dimensionality reduction, visualization and analysis of higher-dimensional data.
Sometimes also referred to as \textit{Kohonen map} or \textit{network}, it was
introduced in 1981 by Teuvo Kohonen \citep{kohonen1990}.
\\
The \gls{som} is a
variant of an \textit{artificial neural network} that uses an unsupervised,
competitive learning process to map a set of higher-dimensional observations
(the \textit{input vectors}) onto a regular, often two-dimensional grid or \textit{map}
of \textit{neurons} or \textit{nodes} that is easy to visualize.
The \gls{som} can be regarded as a nonlinear generalization of a principal
component analysis (PCA) \citep{yin2007} or as a quantization of the input data,
with the nodes along the map functioning as pointers into that
higher-dimensional space. Each node has a position on the lower-dimensional grid
as well as an associated position in the input space, which takes the form of a
$n$-dimensional weight vector $m = [m_{1}, ... , m_{n}]$, where $n$ is the
number of dimensions of the input vectors. Nodes that are in close proximity to
each other on the \gls{som} will also have similar weight vectors
\citep{vesanto2000}, although the inverse (neighboring positions in the input
space also mapping to neighboring nodes) is not necessarily true
\citep{bauer1996}.
\\
For an in-depth look at the algorithm, its variants and applications, as well as
an extensive survey of research on \glspl{som}, the avid reader is referred to
\citet{kohonen2001}.

\subsubsection{Algorithm Definition}
\label{subsubsec:som_math_definition}
The following definition is based on \citet{kohonen1990},
\citet{web:kohonen2005}, \citet{web:kohonen2007} and \citet{bauer1996}.
\bigskip

Consider a space of input data in the form of n-dimensional vectors $x \in
\mathbb{R}^n$ and an ordered set of nodes or model vectors
$m_i \in \mathbb{R}^n$. A vector $x(t)$ is mapped to that node $m_c$ with the
shortest Euclidean distance from it:
\begin{equation}
\label{eq:bmu}
  ||x(t) - m_c|| \leq ||x(t) - m_i|| \; \forall{i}.
\end{equation}
This "winning" node $m_c$ is referred to as the \gls{bmu} for $x(t)$.
\bigskip

During the learning or adaptation phase of the algorithm, all nodes $m_i$ are
adjusted by a recursive regression process
\begin{equation}
  \label{eq:node_update}
  m_i(t+1) = m_i(t) + h_{c(x),i}(x(t) - m_i(t)),
\end{equation}
where $t$ is the index of the current regression step, $x(t)$ is an input vector
chosen randomly from the input data at this step, $c$ is the index of the
\gls{bmu} for the current input vector $x(t)$ according to equation \ref{eq:bmu}
and $h_{c(x),i}$ represents a so-called \textit{neighborhood function}. The
name-giving \textit{neighborhood} is a subset $N_c$ of nodes centered on $m_c$.
At each learning step $t$, those nodes that are within $N_c$ will be adjusted,
whereas those outside of it will not. The reason for employing such a
neighborhood function is so that the nodes "doing the learning are not affected
independently of each other" \citep[p.1467]{kohonen1990} and "the topography of
the map is ensured" \citep[p.5]{bauer1996}. At its most basic, the
neighborhood function is a decreasing distance function between neurons $m_i$
and $m_c$. Its most common form, which is also employed in this work, is that of
a Gaussian function with its peak at $m_c$ such that
\begin{equation}
  h_{c(x),i} = \alpha(t) \exp\bigg(- \frac{||r_i - r_c||^2}{2\sigma^2(t)}\bigg).
\end{equation}
Here, $\alpha$ denotes a learning rate factor or adaptation "gain control"
$0 < \alpha(t) < 1$, which decreases over the course of the regression,
$r_i \in \mathbb{R}^2$ and $r_c \in \mathbb{R}^2$ are the locations of $m_i$ and
$m_c$ on the \gls{som} grid (the lower-dimensional output map, not the input
space!), and $\sigma(t)$ is the width of the neighborhood function, which again
decreases as the regression step index increases.
\bigskip

% TODO Algorithm description using pseudo-code???

\subsubsection{Node Initialization}
\label{subsubsec:som_initialization}
Because of the iterative nature of the \gls{som} algorithm, its outcome depends
on the initial positions chosen for the nodes. The method implemented in this
work uses random initialization, meaning the starting positions of the nodes are
chosen randomly from within the bounds of the input space. An often employed
alternative approach is to first perform a \gls{pca} on the input data, select
the largest $d$ components, where $d$ is the number of desired output dimensions
for the \gls{som}, and then distribute the nodes at equidistant intervals along
those component vectors.

\subsubsection{Input Data Scaling}
\label{subsubsec:som_input_normalization}
Some consideration should be given to the dynamic range of the input data across
its different dimensions. Are the dimensional ranges comparable in their limits?
What about their variance? There does not appear to exist a clear consensus
across the literature on whether or not normalization of input data is strictly
necessary (\citet[p.34]{vesanto2000}, \citet[p.1470]{kohonen1990},
\citet{web:kohonen2007}).

Because the range of the data derived from the audio
feature analysis used in this work varies considerably between features, the
data for feature $n$ is rescaled to have unit variance by dividing by the
features' standard deviation $\sigma_n$:
\begin{equation}
  x_{n} = \frac{x_{n}}{\sigma_n}.
\end{equation}

\subsubsection{Alternative Learning Rate Factors}
\label{subsubsec:som_learning_rates}

\paragraph*{Linear}
\label{para:alpha_linear}
The traditional \gls{som} algorithm uses a learning rate factor $\alpha$ that
decreases linearly as a function of the regression step $t$:
\begin{equation}
  \alpha(t) = \alpha_0 \bigg(\frac{1 - t}{T}\bigg),
\end{equation}
where $\alpha_0$ is the initially chosen learning rate and $T$ is the total
training length or number of regression steps.
\bigskip

Two other approaches to the decreasing learning rate factor were implemented
in this work:

\paragraph*{Inverse}
\label{para:alpha_inverse}
The first is a reciprocally decreasing function where
\begin{equation}
  \alpha(t) = \frac{\alpha_0}{\big(\frac{1 + 100t}{T}\big)}.
\end{equation}

\paragraph*{\gls{bdh}}
\label{para:alpha_bdh}
The second alternative approach is that of an adaptive local learning rate as
developed by \citet{bauer1996} (referred to hereafter as \gls{bdh}, also see \citet{merenyi2007}):
\begin{equation}
  \alpha(t) = \alpha_0 \bigg(\frac{1}{\Delta{t_c}}\bigg(\frac{1}{|x(t)-m_c|^n}\bigg)\bigg)^m,
\end{equation}
where $\Delta{t_c}$ represents the time since the current \gls{bmu} for the
current input vector was last selected as a \gls{bmu} for any vector and $m$ is
a newly introduced, free control parameter. For a more complete review of the
uses of this algorithm, the reader is referred to the original paper
\citep{bauer1996} as well as \citet{merenyi2007}.
