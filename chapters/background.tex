% !TEX root = ../thesis.tex

\section{Background}
\label{sec:background}
This is the Background section.

\subsection{Audio Feature Extraction}
\label{subsec:feature_extraction}
% TODO
Make sure to quote \citet{lerch2012}, \citet{rawlinson2015},
\citet{web:meyda2019}, \citet{mathieu2010} \citet{web:yaafe2019}.

\subsubsection{Fundamentals}
\label{subsubsec:feature_fundamentals}

\subsubsection{Audio Pre-Processing}
\label{subsubsec:preprocessing}

\subsubsection{Time-Domain Features}
\label{subsubsec:temporal_features}
% TODO
Define $x[n]$, $n$

\paragraph{Duration}
\label{para:duration}
The overall duration of the signal $x[n]$ in seconds:
\begin{equation}
  v_{DUR} = \frac{n}{fs}s,
\end{equation}
where $n$ is the number of samples and $fs$ is the sampling rate.

\paragraph{Root Mean Square (RMS)}
\label{para:rms}
measures the power of a signal \citep[p.73f]{lerch2012}. It describes sound intensity and is sometimes used as a simple measure for loudness
\citep{web:meyda2019_features} that does not take the nonlinearity of human
hearing into account \citep{fletcher1933}. It is calculated for an audio
frame $x[n]$ consisting of $n$ samples such that
\begin{equation}
  v_{RMS} = \sqrt{ \frac{ \sum\limits_{i=1}^{n} x(i)^2} {n}}.
\end{equation}

\paragraph{Zero-Crossing Rate (ZCR)}
\label{para:zcr}
represents the rate of the number of sign changes in a signal.
It can be used as a measure of the tonalness of a sound \citep{lykartsis2014}
and as a simple pitch detection method for monophonic signals \citep{web:pitchdetection2019}. It is defined as
\begin{equation}
  v_{ZCR} = \frac{1} {2\cdot{n}}\sum\limits_{i=1}^{n} |sgn[x(i)] - sgn[x(i - 1)]|.
\end{equation}

\subsubsection{Frequency-Domain Features}
\label{subsubsec:spectral_features}
% TODO
Define $N_{FFT}$, $X(k)$

\paragraph{Spectral Centroid}
\label{para:centroid}
is a measure of the center of gravity of a
spectrum. A higher value indicates a brighter, sharper sound \citep{lerch2012}.
The spectral centroid is defined as
\begin{equation}
  v_{SC} = \frac{ \sum\limits_{k=0}^{N_{FFT}/2-1} k\cdot{|X(k)]^2} }
  { \sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)]^2 }.
\end{equation}

\paragraph{Spectral Flatness}
\label{para:flatness}
is a measure for the tonality or noisiness of a signal, defined as the ratio of
the geometric and arithmetic means of its magnitude spectrum. Higher values
indicate a flatter (and therefore noisier) spectrum, whereas lower values point
towards more tonal spectral content. It is defined as
\begin{equation}
  v_{SFL} = \frac{ \sqrt[N_{FFT}/2]{ \prod\limits_{k=0}^{N_{FFT}/2-1} |X(k)| } }
  { (2/N_{FFT}) \cdot{\sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)|} }.
\end{equation}

\paragraph{Spectral Kurtosis}
\label{para:kurtosis}
indicates whether a given magnitude spectrum's distribution is similar to a
Gaussian distribution. Negative values result from a flatter distribution,
whereas positive values indicate a peakier distribution. A Gaussian
distribution would result in a value of 0. Spectral Kurtosis is defined as
\begin{equation}
  v_{SKU} = \frac{ 2 \sum\limits_{k=0}^{N_{FFT}/2-1} (|X(k)|
  - \mu_{|X|})^4 }
  { N_{FFT}\cdot{\sigma_{|X|}^4} } - 3,
\end{equation}

where $\mu_{|X|}$ represents the mean and $\sigma_{|X|}$ the standard deviation
of the magnitude spectrum $|X|$.

\paragraph{Spectral Skewness}
\label{para:skewness}
assesses the symmetry of a magnitude spectrum distribution. It is defined as
\begin{equation}
  v_{SSK} = \frac{ 2 \sum\limits_{k=0}^{N_{FFT}/2-1} (|X(k)|
  - \mu_{|X|})^3 }
  { N_{FFT}\cdot{\sigma_{|X|}^3} }.
\end{equation}

\paragraph{Spectral Slope}
\label{para:slope}
represents a measure of how sloped or inclined a given spectral distribution is.
The spectral slope is calculated using a linear regression of the magnitude
spectrum such that
\begin{equation}
  v_{SSL} = \frac{ \sum\limits_{k=0}^{N_{FFT}/2-1} (k - \mu_{k}) (|X(k)|
  - \mu_{|X|}) }
  { \sum\limits_{k=0}^{N_{FFT}/2-1} (k - \mu_{k})^2 }.
\end{equation}

\paragraph{Spectral Spread}
\label{para:spread}
is a descriptor of the concentration of a magnitude spectrum around the
\nameref{para:centroid} and assesses the corresponding signal's bandwidth. It is
defined as
\begin{equation}
  v_{SSP} = \frac{ \sum\limits_{k=0}^{N_{FFT}/2-1} (k - v_{SC})^2
  \cdot{ |X(k)|^2 } }
  { \sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)|^2 }.
\end{equation}

\paragraph{Spectral Rolloff}
\label{para:rolloff}
measures the bandwidth of a given signal by calculating that frequency bin
below which lie $\kappa$ percent of the sum of magnitudes of $X(k)$. Common
values for $\kappa$ are $0.85$, $0.95$ \citep{lerch2012} or $0.99$
\citep{web:meyda2019_features}. It is defined as
\begin{equation}
  v_{SR} = i \Bigg\rvert_{ \sum\limits_{k=0}^{i} |X(k)| = \kappa
  \cdot{ \sum\limits_{k=0}^{N_{FFT}/2-1} |X(k)| } }.
\end{equation}

\subsubsection{Perceptual Features}
\label{perceptual_features}

\paragraph{Total Loudness}
\label{para:loudness}
represents an algorithmic approximation of the human perception of a signal's
loudness based on \citet{moore1997}, which uses the Bark scale as introduced by
\citet{zwicker1961}. The Total Loudness is the sum of all 24 bands' specific loudness
coefficients, defined by \citet{peeters2004} as
\begin{equation}
  v_{TL} = \sum\limits_{i=1}^{24} v_{SL}(i),
\end{equation}
where
\begin{equation}
  v_{SL}(i) = E(i)^{0.23}
\end{equation}
is the specific loudness of each Bark band (see \citet{moore1997} for further
details).

\subsection{Self-Organzing Map}
\label{subsec:som}

The \textit{self-organizing map} (\gls{som}) is a machine learning algorithm for
dimensionality reduction, visualization and analysis of higher-dimensional data.
Sometimes also referred to as \textit{Kohonen map} or \textit{network}, it was
introduced in 1981 by Teuvo Kohonen \citep{kohonen1990}.
\\
The \gls{som} is a
variant of an \textit{artificial neural network} that uses an unsupervised,
competitive learning process to map a set of higher-dimensional observations
(the \textit{input vectors}) onto a regular, often two-dimensional grid or \textit{map}
of \textit{neurons} or \textit{nodes} that is easy to visualize.
The \gls{som} can be regarded as a nonlinear generalization of a principal
component analysis (PCA) \citep{yin2007} or as a quantization of the input data,
with the nodes along the map functioning as pointers into that higher-dimensional
space. Each node has a position on the lower-dimensional grid as well as an
associated position in the input space, which takes the form of a
$d$-dimensional weight vector $m = [m_{1}, ... , m_{d}]$, where $d$ is the
number of dimensions of the input vectors. Nodes that are in close proximity to
each other on the \gls{som} will also have similar weight vectors
\citep{vesanto2000}.
\\
For an in-depth look at the algorithm, its variants and applications, as well as
an extensive survey of research on \glspl{som}, the avid reader is referred to
\citet{kohonen2001}.

\smallskip

what is it used for?
pattern recognition
\smallskip

mathematical definition
\smallskip

algorithm described using pseudo-code

initialization

input data normalization
