% !TEX root = ../thesis.tex

\section{Evaluation}
\label{sec:evaluation}
% TODO: This.
In order to evaluate the \gls{som} algorithm as implemented in this thesis, as
well as the developed SOM Browser application as a whole, a two-part process
was employed. First, a set of numerical metrics was selected to quantify aspects
of the algorithm we deem salient when judging its effectiveness for sound corpus
organization. Second, a series of five semi-structured interviews was designed,
conducted and subsequently analyzed. The following sections go into detail about
the selection of a data set of sound files for the evaluation, the metrics
employed and the design of the interview.

\subsection{Sound Corpus Selection}
\label{subsec:eval_corpus_selection}
A crucial aspect for the evaluation of the work presented in this thesis is the
choice of an appropriate data set of audio files to serve as a prototypical
sound corpus. Ideally, two key conditions should be met by this corpus. It
should be \textit{ecologically valid}, meaning here that it should approximate a
real-world sample library that would actually be used by contemporary music
producers, and it should be a \textit{well-established} data set which has been
validated through use in other research, allowing for direct comparisons between
results. Preferably, something akin to the Giant Steps data sets
\citep{knees2015} for tempo and key detection should be used.
In addition to identifying the aforementioned two conditions, the decision was
made to only select "one-shot" drum and percussion sounds (meaning single
instrument hits, no loops or other longer sounds) in order to evaluate a single,
concrete use case and limit the scope of this evaluation.

\smallskip

Data sets used in previous research vary and it is often not possible to clearly
establish provenance due to insufficient information being given by the authors
(see for example \citet{fried2014} and \citet{shier2017}, two papers which
present important related work but fail to clearly identify the source of their
employed sound files). Two established databases that have been cited in the
literature are ENST-Drums \citep{gillet2006} and the RWC Music Database
\citep{goto2002}. However, neither of these data sets proved appropriate for
this evaluation since they both contain only acoustic source material and,
especially in the case of RWC, largely consist of longer musical passages
instead of the single "one-shot" hits mentioned above.

\smallskip

For the reasons outlined above, the author decided to forego the condition that
the selected sound corpus be a data set well-established through previous
research. Because of this, more emphasis is placed on the requirement for
ecological validity. In order to maximize real-world conditions, the sample
library \textit{Drum Essentials} \citep{drumessentials2019} was selected to
serve as a sound corpus for this evaluation. It is a collection of samples
created by the German music software company Ableton AG that is distributed to
owners of the company's flagship product, the \gls{daw} \textit{Ableton Live}
\citep{abletonlive2019}. As part of a commercially available product, this
corpus of sound files does not just approximate a real-world sample library, it
is an actual example of such a library and is, for the purpose of this thesis,
considered representative of sample libraries used in a modern music production
workflow. One additional benefit of using the selected sample library is the
advantage of a single, clearly identifiable source of the data - it is made
available as a professional product by Ableton AG. An alternative approach would
have been to manually select sounds from places like \textit{Freesound.org}
\citep{font2013}, where all files are licensed in a way that makes them free to
use, but their quality is not guaranteed to be consistent, or to scour through
sample libraries shared on various online forums, which brings along issues of
copyright and expired links, making it hard to trace the files' origins.

\smallskip

The \textit{Drum Essentials} collection as distributed by Ableton consists of
1181 one-shot samples, each in a separate audio file, as well as supplementary
content, such as MIDI clips and effects presets. Only the raw audio files are
used in the presented work. These sound files present a mixture of acoustic
and electronic sounds stemming from a variety of drums and percussion
instruments. The library is organized by instrument group, of which there are
17 in total. The names of these groups, as well as the number of sounds per
group can be found in table \ref{table:drum_essentials_counts}. Some sound files
appear in more than one group. These duplicates have been removed, so that every
sound only appears once throughout the entire data set. The remaining number of
sound files is 1081.

\renewcommand{\arraystretch}{1.2}

\begin{table}[!ht]
  \renewcommand{\arraystretch}{1.2}
  \centering
  \footnotesize
  \rowcolors{2}{table-bg-one}{table-bg-two}
  \begin{tabular}{ l | c }
    % \hline
    \multicolumn{2}{ c }{\textbf{Drum Essentials}} \\
    \hline
    \textbf{Instrument Category} & \textbf{Count} \\
    \hline
    Bell & 19 \\
    Bongo & 6 \\
    Clap & 71 \\
    Conga & 27 \\
    Cymbal & 54 \\
    Electronic Percussion & 49 \\
    FX Hit & 64 \\
    Hihat & 167 \\
    Kick & 166 \\
    Misc. Percussion & 64 \\
    Ride & 40 \\
    Rim & 65 \\
    Shaker & 39 \\
    Snare & 181 \\
    Tambourine & 23 \\
    Tom & 138 \\
    Wood & 8 \\
    % \hline
  \end{tabular}
  \caption{Sound file counts per instrument category of the
  \textit{Drum Essentials} sample library}
  \label{table:drum_essentials_counts}
\end{table}

\subsection{Metrics for SOM Analysis}
\label{subsec:eval_som_metrics}
In order to evaluate the \glspl{som} created using the SOM Browser application
and the \textit{Drum Essentials} test data set, three core metrics are used:
quantization induced by the \gls{som}, map emptiness, and the ratio between
nodes and their assigned vectors. These metrics and the motivation behind them
are outlined further in the following paragraphs.

\subsubsection{SOM-Induced Quantization}
\label{subsubsec:som_quantization}
Fundamental to the \gls{som} principle is the idea of mapping vectors to their
corresponding \glspl{bmu}, those nodes that are closest to them (see section
\ref{subsec:som}). Several vectors can be assigned to one node - this can also
be thought of as a quantization process, where the magnitude of the difference
between the positions of vector $ x_t $ and node $ m_c $ is the quantization
error $ \Delta_t $ for that vector:

\begin{equation}
  \Delta_t = || x_t - m_c ||
\end{equation}

As a metric for the \gls{som}, the quantization errors for all vectors can be
averaged, as well as their distribution examined. In order to maximize
information preservation, quantization errors should be minimized.

\subsubsection{Vector-Node Count}
\label{subsubsec:vector_node_count}
A second metric that was devised in order to quantify \gls{som} quality is the
count $ C_i $ of vectors $ x_1, ... , x_n $ mapped to a node $ m_i $ and
subsequently the distribution of those counts across the map. Ideally, this
distribution should look like a single, narrow spike - meaning that (almost) all
nodes have about the same number of vectors assigned to them, resulting in an
even distribution of sounds across the SOM Browser map interface.

\subsubsection{Map Emptiness}
\label{subsubsec:map_emptiness}
Another relevant aspect of the created \glspl{som}, and the third metric
employed here, is how much of the map remains "empty", meaning how many nodes
were not assigned any vectors. We define this "map emptiness" metric $ ME $ as
the number of nodes $ m_1, ... m_n $ whose vector-node count $ C_n = 0 $ (see
section \ref{subsubsec:vector_node_count}), divided by the total number of nodes
$ m_i $. For the purpose of making optimal use of the space alloted to the map
in the SOM Browser \gls{gui}, emptiness should be minimized so that users
encounter the least amount of "blind spots" possible.

\subsubsection{Influence of Forced Node Population}
\label{subsubsec:eval_fnp_influence}
Since the concept of Forced Neuron Population is an addition to the \gls{som}
algorithm introduced in this work (see section
\ref{subsubsec:som_forced_population}), its influence on the \gls{som} should
also be evaluated. Therefore, the aforementioned metrics were calculated
both with and without \gls{fnp}.

% \subsection{Online Sound Similarity Survey}
% \label{subsec:evaluation_survey}
% Maybe not even do this bit???

\subsection{Semistructured User Interviews}
\label{subsec:evaluation_interviews}

In order to evaluate the SOM Browser application prototype presented in this
thesis, five semi-structured interviews with working audio professionals were
conducted. These interviews were conducted by the author and consisted of a set
of questions as well as observed user interaction with the prototype software.
For this evaluation, a guide including questions outlining the
structure of the interview as well a set of ratings scales was created. Subjects
were asked about their experience with sample libraries and their current
workflow, and to interact with a sample library in a file browser environment as
well as using the SOM Browser software. Audio from the conversations was
recorded and subsequently analyzed.

\subsubsection{Motivation to Conduct Interviews}
\label{subsubsec:interview_motivation}

This evaluation procedure entails two aspects, namely a semi-structured
interview series and a qualitative analysis of the collected responses. The
decision to conduct qualitative interviews stems from the exploratory nature of
the presented work. In order to assess the merit of the developed interface in
its present state, direct feedback from potential users was sought, which
\citet{lazar2017} refers to as "fundamental to human-computer-interaction (HCI)
research" (see \citet[p.187]{lazar2017}). But the motivation for a direct
conversation with users was not only to evaluate the presented interface
proposition, but also for these interviews to serve as an exploration of users'
current situation, to hear about their own experience of it and to see what
advantages and shortcomings they identify in their present workflows. In short,
these interviews were motivated by a desire to gain some understanding of the
complex situation that is sample library interaction in a music production
environment and to gauge initial reactions to the developed prototype
alternative. The semi-structured approach was chosen in order to be able to
react to interviewees' responses more freely and allow the interviewer to ask
follow up questions when deemed necessary. Naturally then, the gathered
responses cannot simply be quantified, which makes a qualitative approach to
their analysis a fitting choice.

\smallskip

There are of course downsides to the chosen approach. Conducting
interviews is time-consuming, as it has to be done on a one-on-one basis and
often (as in the case of this work) in person. After the interview is over,
additional time and effort goes into transcribing and annotating the responses.
This severely limits the number of participants that can feasible be recruited
for a study, as is evident by the small number of five participants here.
\citet{lazar2017} identifies another disadvantage of interviews: "[...] data
collection that is separated from the task and context under consideration [...]
suffer[s] from problems of recall. [...] [I]t is, by definition, one step
removed from reality" \citep[p.188ff.]{lazar2017}. Because of this, we follow
the authors' suggestion of combining the interview with user observation.

\subsubsection{Interview Subject Selection}
\label{subsubsec:subject_selection}
The SOM Browser application is not aimed at the general population. Instead, it
has been designed for specialized users that work in modern music production, as
they constitute the potential future user base of an application like the one
presented here.

\smallskip

In order to increase the validity and relevance of potential subjects'
responses, the decision was made to interview only working professionals for
this evaluation and to not include hobbyists or people without any experience in
music production.

\smallskip

Subjects were recruited by inquiring about qualified candidates (in other words,
people working professionally in modern music production) in the wider circle of
acquaintances of the author. No compensation was offered and only sparse
information about the nature of the research was given beforehand in order to
minimize the possibility of instilling biases in subjects. Most importantly,
subjects were asked to participate in an interview about sample library
organization, but were not told that they would be shown software developed by
the author.

\subsubsection{Informed Consent Form}
\label{subsubsec:consent_form}
For the purpose of documenting participants agreement to be interviewed, an
informed consent form was created for the interview series. This document
outlines basic information about the purpose and content of the interview and
its duration. It also lists all data that will be collected and explains the
procedure used for data anonymization in order to protect subjects' privacy.
Lastly, it informs participants of their rights to withdraw their consent to the
usage of their data for research purposes and have it erased. This form was
based on a template provided by the ethics commission of \gls{tu-berlin} on
their website \citep{web:ethics2019}. The form used by the author can be found
in
% TODO
XXX REF APPENDIX HERE XXX.

\subsubsection{Test Subject Code Design}
\label{subsubsec:subject_code}
To ensure proper data anonymization, a test subject code was used. This code
is comprised of a series of letters and numbers and was created at the beginning
of the interview by the subjects themselves according to a set of instructions.
All data and responses of the subjects were directly labelled with this code, so
that individuals' names were never used. This code design procedure was again
based on a template by the ethics commission of \gls{tu-berlin} and can be found
on the same website as the information concerning consent forms
\citep{web:ethics2019}. The instruction sheet that was distributed to subjects
can be found in
% TODO
XXX REF APPENDIX HERE XXX.

\subsubsection{Interview Structure}
\label{subsubsec:interview_structure}
The guide developed for this interview can be found in
% TODO
XXX REF APPENDIX HERE XXX
It outlines a three part structure: first, some general questions about
subjects' usage of sample libraries. Second, some guided interaction with a
predetermined sample library in a traditional file browser structure on a
computer. In the third section, the SOM Browser application is finally
introduced and subjects are asked to use it and describe their impression of it.

\subsubsection{Question Design}
\label{subsubsec:question_design}
The general composition employed for most questions is twofold, combining
closed- and open-ended approaches: first,
participants are asked to give a rating on a predefined scale (see
\ref{subsubsec:ratings_scales} below).
Then, participants are free to elaborate on their answer and explain their
rating. If they don't initiate this themselves, a follow-up question along the
lines of "Could you tell me why you chose this rating?" is asked.

\subsubsection{Selection of Ratings Scales}
\label{subsubsec:ratings_scales}
In order to record subjects' ratings, 6 point Likert scales were used (as is
common in \gls{hci} research, see \citet[p.31, p.93]{lazar2017}). The difference
between even and uneven anchor counts in Likert scales lies in the presence (in
the case of uneven anchor counts) or lack (for even counts) of a "neutral"
middle option. Choosing scales without neutral mid-points was motivated by a
desire to encourage subjects to make a definite choice with regard to their
rating. For a short look at the effects of eliminating the mid-point, see
\citet{garland1991}.
\smallskip
The scales presented to subjects were explicitly labeled textually instead of
numerically. The anchor points were designed using two polar adjectives (such as
"positive" and "negative") and a consistent, three-tiered set of adjective
qualification with "very" marking the strongest option, followed by the
adjective without qualifier and then "somewhat" as the weakest variant. The
resulting scale for a positive/negative rating is composed of the following
anchors: very positive, positive, somewhat positive, somewhat negative,
negative, very negative. The selection of these qualifiers and appropriate
anchors in general was inspired partially by \citet{vagias2006}. The full set of
scales used for the conducted interviews can be found in
% TODO
XXX REF APPENDIX HERE XXX.

\subsubsection{Questions Used}
\label{subsubsec:questions_used}
In section 1, which serves as an introduction for the interviewee, general
administrative requirements such as the signing of the consent form and a
topical introduction of the research are taken care off. This is then followed
by two simple Yes/No questions to establish whether the subject works with
third-party and personally created sample libraries (see questions 1.1 and
1.2).

\bigskip

Section 2 begins with a presentation of the \textit{Drum Essentials} sample
library to the subject. This presentation includes the information that it is a
library of drum samples that consists of around 1000 sound files which are
organized in subfolders according to the respective instrument, such as kick
drum, snare drum, hi-hat, and so forth. The interviewee is invited to explore
the sample library using the laptop that it is being presented on.

\par
Then, in
question 2.1, subjects are asked to describe how to approach familiarizing
themselves with the provided sample library in order to use its contents in a
hypothetical work project of theirs.

\par
Question 2.2 follows this up with a
request for a rating of the subject's level of satisfaction with the workflow
that they outlined.

\bigskip

In the third and final section of the interview, the SOM Browser software is
introduced to participants. At first, a general overview of the interface is
given, in which the interviewer mentions the map layout in the middle (without
explaining the nature of its organization), the file list on the left, the file
info panel on the right and the favorites bar at the bottom.

\par
The subject is
then asked to try out the software and explore its interface for a short period
of time. Thereafter, they are asked to give a rating of their overall first
impression of the software on a positive/negative scale (see question 3.1).
Then, a follow-up question about their opinion on what does or does not work is
posed.

\par
In question 3.2, subjects are required to rate the interface's ease of
use.

\par
Question 3.3 inquires specifically about the understandability of the
language used.

\par
3.4 and 3.5 are open-ended questions aimed at subjects'
interpretation of the organization of sounds in the map layout: 3.4. asks what
subjects think about the organization, while 3.5 inquires specifically about
a guess as to what the axes represent.

\par
Question 3.6 then asks subjects to state whether or not they have a preference
between the traditional file browser layout presented in section 2 or the SOM
Browser interface shown in section 3.

\par
The last ratings question of the interview, 3.7 requests interviewees to assess
their level of comfortability with the software.

\par
Finally, in 3.8 subjects are asked if they would consider using the presented
software tool and what changes they would like to see.

\bigskip

The full interview guide including all questions can be found in
% TODO
XXX REF APPENDIX HERE XXX.
